{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1grVn3xdDq4_y3wzP1DJwddLMS5Y1KjNF","authorship_tag":"ABX9TyOO6cquhu45YOTzz3WQMm3/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"metadata":{"id":"KoxQRB3L0whH","executionInfo":{"status":"ok","timestamp":1674819644935,"user_tz":-420,"elapsed":168,"user":{"displayName":"Hùng Phạm Nguyên","userId":"11590122392146338072"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"zAWO8mu_8ONE","executionInfo":{"status":"ok","timestamp":1674819645449,"user_tz":-420,"elapsed":316,"user":{"displayName":"Hùng Phạm Nguyên","userId":"11590122392146338072"}}},"outputs":[],"source":["class SelfAttention(nn.Module):\n","    def __init__(self, channels, size):\n","        super(SelfAttention, self).__init__()\n","        self.channels = channels\n","        self.size = size\n","        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n","        self.ln = nn.LayerNorm([channels])\n","        self.ff_self = nn.Sequential(\n","            nn.LayerNorm([channels]),\n","            nn.Linear(channels, channels),\n","            nn.GELU(),\n","            nn.Linear(channels, channels),\n","        )\n","\n","    def forward(self, x):\n","        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n","        x_ln = self.ln(x)\n","        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n","        attention_value = attention_value + x\n","        attention_value = self.ff_self(attention_value) + attention_value\n","        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)\n","\n","\n","class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n","        super().__init__()\n","        self.residual = residual\n","        if not mid_channels:\n","            mid_channels = out_channels\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n","            nn.GroupNorm(1, mid_channels),\n","            nn.GELU(),\n","            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n","            nn.GroupNorm(1, out_channels),\n","        )\n","\n","    def forward(self, x):\n","        if self.residual:\n","            return F.gelu(x + self.double_conv(x))\n","        else:\n","            return self.double_conv(x)\n","\n","\n","class Down(nn.Module):\n","    def __init__(self, in_channels, out_channels, emb_dim=256):\n","        super().__init__()\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            DoubleConv(in_channels, in_channels, residual=True),\n","            DoubleConv(in_channels, out_channels),\n","        )\n","\n","        self.emb_layer = nn.Sequential(\n","            nn.SiLU(),\n","            nn.Linear(\n","                emb_dim,\n","                out_channels\n","            ),\n","        )\n","\n","    def forward(self, x, t):\n","        x = self.maxpool_conv(x)\n","        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n","        return x + emb\n","\n","\n","class Up(nn.Module):\n","    def __init__(self, in_channels, out_channels, emb_dim=256):\n","        super().__init__()\n","\n","        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n","        self.conv = nn.Sequential(\n","            DoubleConv(in_channels, in_channels, residual=True),\n","            DoubleConv(in_channels, out_channels, in_channels // 2),\n","        )\n","\n","        self.emb_layer = nn.Sequential(\n","            nn.SiLU(),\n","            nn.Linear(\n","                emb_dim,\n","                out_channels\n","            ),\n","        )\n","\n","    def forward(self, x, skip_x, t):\n","        x = self.up(x)\n","        x = torch.cat([skip_x, x], dim=1)\n","        x = self.conv(x)\n","        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n","        return x + emb"]},{"cell_type":"code","source":["class UNet(nn.Module):\n","    def __init__(self, c_in=3, c_out=3, time_dim=256, device=\"cuda\"):\n","        super().__init__()\n","        self.device = device\n","        self.time_dim = time_dim\n","        self.inc = DoubleConv(c_in, 64)\n","        self.down1 = Down(64, 128)\n","        self.sa1 = SelfAttention(128, 32)\n","        self.down2 = Down(128, 256)\n","        self.sa2 = SelfAttention(256, 16)\n","        self.down3 = Down(256, 256)\n","        self.sa3 = SelfAttention(256, 8)\n","\n","        self.bot1 = DoubleConv(256, 512)\n","        self.bot2 = DoubleConv(512, 512)\n","        self.bot3 = DoubleConv(512, 256)\n","\n","        self.up1 = Up(512, 128)\n","        self.sa4 = SelfAttention(128, 16)\n","        self.up2 = Up(256, 64)\n","        self.sa5 = SelfAttention(64, 32)\n","        self.up3 = Up(128, 64)\n","        self.sa6 = SelfAttention(64, 64)\n","        self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n","\n","    def pos_encoding(self, t, channels):\n","        inv_freq = 1.0 / (\n","            10000\n","            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n","        )\n","        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n","        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n","        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n","        return pos_enc\n","\n","    def forward(self, x, t):\n","        t = t.unsqueeze(-1).type(torch.float)\n","        t = self.pos_encoding(t, self.time_dim)\n","\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1, t)\n","        x2 = self.sa1(x2)\n","        x3 = self.down2(x2, t)\n","        x3 = self.sa2(x3)\n","        x4 = self.down3(x3, t)\n","        x4 = self.sa3(x4)\n","\n","        x4 = self.bot1(x4)\n","        x4 = self.bot2(x4)\n","        x4 = self.bot3(x4)\n","\n","        x = self.up1(x4, x3, t)\n","        x = self.sa4(x)\n","        x = self.up2(x, x2, t)\n","        x = self.sa5(x)\n","        x = self.up3(x, x1, t)\n","        x = self.sa6(x)\n","        output = self.outc(x)\n","        return output"],"metadata":{"id":"mJeUd0P_114a","executionInfo":{"status":"ok","timestamp":1674819645449,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hùng Phạm Nguyên","userId":"11590122392146338072"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import torchvision\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","from torch.utils.data import DataLoader\n","\n","\n","def plot_images(images):\n","    plt.figure(figsize=(32, 32))\n","    plt.imshow(torch.cat([\n","        torch.cat([i for i in images.cpu()], dim=-1),\n","    ], dim=-2).permute(1, 2, 0).cpu())\n","    plt.show()\n","\n","\n","def save_images(images, path, **kwargs):\n","    grid = torchvision.utils.make_grid(images, **kwargs)\n","    ndarr = grid.permute(1, 2, 0).to('cpu').numpy()\n","    im = Image.fromarray(ndarr)\n","    im.save(path)\n","\n","\n","def get_data(dataset_path, image_size, batch_size):\n","    transforms = torchvision.transforms.Compose([\n","        torchvision.transforms.Resize(80),  # args.image_size + 1/4 *args.image_size\n","        torchvision.transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n","        torchvision.transforms.ToTensor(),\n","        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","    dataset = torchvision.datasets.ImageFolder(dataset_path, transform=transforms)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","    return dataloader\n","\n","\n","def setup_logging(run_name):\n","    os.makedirs(\"models\", exist_ok=True)\n","    os.makedirs(\"results\", exist_ok=True)\n","    os.makedirs(os.path.join(\"models\", run_name), exist_ok=True)\n","    os.makedirs(os.path.join(\"results\", run_name), exist_ok=True)"],"metadata":{"id":"LbjzLljY3Kln","executionInfo":{"status":"ok","timestamp":1674819645450,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hùng Phạm Nguyên","userId":"11590122392146338072"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm\n","from torch import optim\n","import logging\n","from torch.utils.tensorboard import SummaryWriter\n","\n","logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")\n","\n","\n","class Diffusion:\n","    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=256, device=\"cuda\"):\n","        self.noise_steps = noise_steps\n","        self.beta_start = beta_start\n","        self.beta_end = beta_end\n","        self.img_size = img_size\n","        self.device = device\n","\n","        self.beta = self.prepare_noise_schedule().to(device)\n","        self.alpha = 1. - self.beta\n","        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n","\n","    def prepare_noise_schedule(self):\n","        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n","\n","    def noise_images(self, x, t):\n","        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n","        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n","        Ɛ = torch.randn_like(x)\n","        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ\n","\n","    def sample_timesteps(self, n):\n","        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n","\n","    def sample(self, model, n):\n","        logging.info(f\"Sampling {n} new images....\")\n","        model.eval()\n","        with torch.no_grad():\n","            x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n","            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n","                t = (torch.ones(n) * i).long().to(self.device)\n","                predicted_noise = model(x, t)\n","                alpha = self.alpha[t][:, None, None, None]\n","                alpha_hat = self.alpha_hat[t][:, None, None, None]\n","                beta = self.beta[t][:, None, None, None]\n","                if i > 1:\n","                    noise = torch.randn_like(x)\n","                else:\n","                    noise = torch.zeros_like(x)\n","                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n","        model.train()\n","        x = (x.clamp(-1, 1) + 1) / 2\n","        x = (x * 255).type(torch.uint8)\n","        return x"],"metadata":{"id":"KV_ccUe_4RX3","executionInfo":{"status":"ok","timestamp":1674819645450,"user_tz":-420,"elapsed":5,"user":{"displayName":"Hùng Phạm Nguyên","userId":"11590122392146338072"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MQ-sSrPY6cxH","executionInfo":{"status":"ok","timestamp":1674819653743,"user_tz":-420,"elapsed":8298,"user":{"displayName":"Hùng Phạm Nguyên","userId":"11590122392146338072"}},"outputId":"68f30c69-2b97-4fee-a05c-bd3a7c05fa54"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm\n","from torch import optim\n","import logging\n","from torch.utils.tensorboard import SummaryWriter"],"metadata":{"id":"qP1o23HKAr2N","executionInfo":{"status":"ok","timestamp":1674819653743,"user_tz":-420,"elapsed":7,"user":{"displayName":"Hùng Phạm Nguyên","userId":"11590122392146338072"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def train():\n","    image_size = 64\n","    batchsize = 8\n","    lr = 3e-4\n","    epochs = 500\n","    # setup_logging(\"DDPM\")\n","    device = \"cuda\"\n","    dataloader = get_data(dataloader_clean, image_size, batchsize)\n","    model = UNet().to(device)\n","    optimizer = optim.AdamW(model.parameters(), lr=lr)\n","    mse = nn.MSELoss()\n","    diffusion = Diffusion(img_size=image_size, device=device)\n","    # logger = SummaryWriter(os.path.join(\"runs\", args.run_name))\n","    l = len(dataloader)\n","\n","    for epoch in range(epochs):\n","        logging.info(f\"Starting epoch {epoch}:\")\n","        pbar = tqdm(dataloader)\n","        for i, (images, _) in enumerate(pbar):\n","            images = images.to(device)\n","            t = diffusion.sample_timesteps(images.shape[0]).to(device)\n","            x_t, noise = diffusion.noise_images(images, t)\n","            predicted_noise = model(x_t, t)\n","            loss = mse(noise, predicted_noise)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            pbar.set_postfix(MSE=loss.item())\n","            # logger.add_scalar(\"MSE\", loss.item(), global_step=epoch * l + i)\n","\n","        sampled_images = diffusion.sample(model, n=images.shape[0])\n","        save_images(sampled_images, f\"{epoch}.jpg\")\n","        # torch.save(model.state_dict(), os.path.join(\"models\", \"DDPM\", f\"ckpt.pt\"))"],"metadata":{"id":"kjI5CLHm_127","executionInfo":{"status":"ok","timestamp":1674819653743,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hùng Phạm Nguyên","userId":"11590122392146338072"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# train()"],"metadata":{"id":"1xoO5UwuBswx","executionInfo":{"status":"ok","timestamp":1674819653744,"user_tz":-420,"elapsed":7,"user":{"displayName":"Hùng Phạm Nguyên","userId":"11590122392146338072"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import torch\n","import os\n","import torchvision\n","from torchvision.transforms.functional import crop\n","\n","\n","# This script is adapted from the following repository: https://github.com/ermongroup/ddim\n","\n","def compute_alpha(beta, t):\n","    beta = torch.cat([torch.zeros(1).to(beta.device), beta], dim=0)\n","    a = (1 - beta).cumprod(dim=0).index_select(0, t + 1).view(-1, 1, 1, 1)\n","    return a\n","\n","\n","def data_transform(X):\n","    return 2 * X - 1.0\n","\n","\n","def inverse_data_transform(X):\n","    return torch.clamp((X + 1.0) / 2.0, 0.0, 1.0)\n","\n","\n","def generalized_steps(x, x_cond, seq, model, b, eta=0.):\n","    with torch.no_grad():\n","        n = x.size(0)\n","        seq_next = [-1] + list(seq[:-1])\n","        x0_preds = []\n","        xs = [x]\n","        for i, j in zip(reversed(seq), reversed(seq_next)):\n","            t = (torch.ones(n) * i).to(x.device)\n","            next_t = (torch.ones(n) * j).to(x.device)\n","            at = compute_alpha(b, t.long())\n","            at_next = compute_alpha(b, next_t.long())\n","            xt = xs[-1].to('cuda')\n","\n","            et = model(torch.cat([x_cond, xt], dim=1), t)\n","            x0_t = (xt - et * (1 - at).sqrt()) / at.sqrt()\n","            x0_preds.append(x0_t.to('cpu'))\n","\n","            c1 = eta * ((1 - at / at_next) * (1 - at_next) / (1 - at)).sqrt()\n","            c2 = ((1 - at_next) - c1 ** 2).sqrt()\n","            xt_next = at_next.sqrt() * x0_t + c1 * torch.randn_like(x) + c2 * et\n","            xs.append(xt_next.to('cpu'))\n","    return xs, x0_preds\n","\n","\n","def generalized_steps_overlapping(x, x_cond, seq, model, b, eta=0., corners=None, p_size=None, manual_batching=True):\n","    with torch.no_grad():\n","        n = x.size(0)\n","        seq_next = [-1] + list(seq[:-1])\n","        x0_preds = []\n","        xs = [x]\n","\n","        x_grid_mask = torch.zeros_like(x_cond, device=x.device)\n","        for (hi, wi) in corners:\n","            x_grid_mask[:, :, hi:hi + p_size, wi:wi + p_size] += 1\n","\n","        for i, j in zip(reversed(seq), reversed(seq_next)):\n","            t = (torch.ones(n) * i).to(x.device)\n","            next_t = (torch.ones(n) * j).to(x.device)\n","            at = compute_alpha(b, t.long())\n","            at_next = compute_alpha(b, next_t.long())\n","            xt = xs[-1].to('cuda')\n","            et_output = torch.zeros_like(x_cond, device=x.device)\n","            \n","            if manual_batching:\n","                manual_batching_size = 64\n","                xt_patch = torch.cat([crop(xt, hi, wi, p_size, p_size) for (hi, wi) in corners], dim=0)\n","                x_cond_patch = torch.cat([data_transform(crop(x_cond, hi, wi, p_size, p_size)) for (hi, wi) in corners], dim=0)\n","                for i in range(0, len(corners), manual_batching_size):\n","                    outputs = model(torch.cat([x_cond_patch[i:i+manual_batching_size], \n","                                               xt_patch[i:i+manual_batching_size]], dim=1), t)\n","                    for idx, (hi, wi) in enumerate(corners[i:i+manual_batching_size]):\n","                        et_output[0, :, hi:hi + p_size, wi:wi + p_size] += outputs[idx]\n","            else:\n","                for (hi, wi) in corners:\n","                    xt_patch = crop(xt, hi, wi, p_size, p_size)\n","                    x_cond_patch = crop(x_cond, hi, wi, p_size, p_size)\n","                    x_cond_patch = data_transform(x_cond_patch)\n","                    et_output[:, :, hi:hi + p_size, wi:wi + p_size] += model(torch.cat([x_cond_patch, xt_patch], dim=1), t)\n","\n","            et = torch.div(et_output, x_grid_mask)\n","            x0_t = (xt - et * (1 - at).sqrt()) / at.sqrt()\n","            x0_preds.append(x0_t.to('cpu'))\n","\n","            c1 = eta * ((1 - at / at_next) * (1 - at_next) / (1 - at)).sqrt()\n","            c2 = ((1 - at_next) - c1 ** 2).sqrt()\n","            xt_next = at_next.sqrt() * x0_t + c1 * torch.randn_like(x) + c2 * et\n","            xs.append(xt_next.to('cpu'))\n","    return xs, x0_preds"],"metadata":{"id":"9-0wl6biUc07","executionInfo":{"status":"ok","timestamp":1674819653744,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hùng Phạm Nguyên","userId":"11590122392146338072"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import os\n","from os import listdir\n","from os.path import isfile\n","import torch\n","import numpy as np\n","import torchvision\n","import torch.utils.data\n","import PIL\n","import re\n","import random\n","\n","\n","class RainDrop:\n","    def __init__(self):\n","        # self.config = config\n","        self.data_dir = \"/content/drive/MyDrive/Colab Notebooks/DiffusionModel/Dataset\"\n","        self.transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n","        self.batch_size_train = 1\n","        self.batch_size_sampling = 4\n","        self.image_size = 64\n","        self.patch_n = 16\n","\n","    def get_loaders(self, parse_patches=True, validation='raindrop'):\n","        print(\"=> evaluating raindrop test set...\")\n","        train_dataset = RainDropDataset(dir=os.path.join(self.data_dir, 'datasettrain'),\n","                                        n=self.patch_n,\n","                                        patch_size=self.image_size,\n","                                        transforms=self.transforms,\n","                                        filelist=None,\n","                                        parse_patches=parse_patches)\n","        # val_dataset = RainDropDataset(dir=os.path.join(self.config.data.data_dir, 'data', 'raindrop', 'test'),\n","        #                               n=self.config.training.patch_n,\n","        #                               patch_size=self.config.data.image_size,\n","        #                               transforms=self.transforms,\n","        #                               filelist='raindroptesta.txt',\n","        #                               parse_patches=parse_patches)\n","\n","        if not parse_patches:\n","            self.batch_size_train = 1\n","            self.batch_size_sampling = 1\n","\n","        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size_train,\n","                                                   shuffle=True, num_workers=2,\n","                                                   pin_memory=True)\n","        # val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=self.batch_size,\n","        #                                          shuffle=False, num_workers=self.config.data.num_workers,\n","        #                                          pin_memory=True)\n","\n","        # return train_loader, val_loader\n","        return train_loader\n","\n","class RainDropDataset(torch.utils.data.Dataset):\n","    def __init__(self, dir, patch_size, n, transforms, filelist=None, parse_patches=True):\n","        super().__init__()\n","\n","        if filelist is None:\n","            rain_dir = dir\n","            input_names, gt_names = [], []\n","\n","            # Raindrop train filelist\n","            raindrop_inputs = os.path.join(rain_dir, 'rain')\n","            print(raindrop_inputs)\n","            images = [f for f in listdir(raindrop_inputs) if isfile(os.path.join(raindrop_inputs, f))]\n","            print(len(images))\n","            assert len(images) == 96\n","            input_names += [os.path.join(raindrop_inputs, i) for i in images]\n","            gt_names += [os.path.join(os.path.join(rain_dir, 'clean'), i.replace('rain', 'clean')) for i in images]\n","            print(len(input_names))\n","\n","            x = list(enumerate(input_names))\n","            random.shuffle(x)\n","            indices, input_names = zip(*x)\n","            gt_names = [gt_names[idx] for idx in indices]\n","            self.dir = None\n","        else:\n","            self.dir = dir\n","            train_list = os.path.join(dir, filelist)\n","            with open(train_list) as f:\n","                contents = f.readlines()\n","                input_names = [i.strip() for i in contents]\n","                gt_names = [i.strip().replace('input', 'gt') for i in input_names]\n","\n","        self.input_names = input_names\n","        self.gt_names = gt_names\n","        self.patch_size = patch_size\n","        self.transforms = transforms\n","        self.n = n\n","        self.parse_patches = parse_patches\n","        self.showtest = True\n","\n","    @staticmethod\n","    def get_params(img, output_size, n):\n","        w, h = img.size\n","        th, tw = output_size\n","        if w == tw and h == th:\n","            return 0, 0, h, w\n","\n","        i_list = [random.randint(0, h - th) for _ in range(n)]\n","        j_list = [random.randint(0, w - tw) for _ in range(n)]\n","        return i_list, j_list, th, tw\n","\n","    @staticmethod\n","    def n_random_crops(img, x, y, h, w):\n","        crops = []\n","        for i in range(len(x)):\n","            new_crop = img.crop((y[i], x[i], y[i] + w, x[i] + h))\n","            crops.append(new_crop)\n","        return tuple(crops)\n","\n","    def get_images(self, index):\n","        input_name = self.input_names[index]\n","        gt_name = self.gt_names[index]\n","        img_id = re.split('/', input_name)[-1][:-4]\n","        input_img = PIL.Image.open(os.path.join(self.dir, input_name)) if self.dir else PIL.Image.open(input_name)\n","        try:\n","            gt_img = PIL.Image.open(os.path.join(self.dir, gt_name)) if self.dir else PIL.Image.open(gt_name)\n","        except:\n","            gt_img = PIL.Image.open(os.path.join(self.dir, gt_name)).convert('RGB') if self.dir else \\\n","                PIL.Image.open(gt_name).convert('RGB')\n","\n","        if self.parse_patches:\n","            i, j, h, w = self.get_params(input_img, (self.patch_size, self.patch_size), self.n)\n","            input_img = self.n_random_crops(input_img, i, j, h, w)\n","            gt_img = self.n_random_crops(gt_img, i, j, h, w)\n","            outputs = [torch.cat([self.transforms(input_img[i]), self.transforms(gt_img[i])], dim=0)\n","                       for i in range(self.n)]\n","            return torch.stack(outputs, dim=0), img_id\n","        else:\n","            # Resizing images to multiples of 16 for whole-image restoration\n","            wd_new, ht_new = input_img.size\n","            if ht_new > wd_new and ht_new > 1024:\n","                wd_new = int(np.ceil(wd_new * 1024 / ht_new))\n","                ht_new = 1024\n","            elif ht_new <= wd_new and wd_new > 1024:\n","                ht_new = int(np.ceil(ht_new * 1024 / wd_new))\n","                wd_new = 1024\n","            wd_new = int(16 * np.ceil(wd_new / 16.0))\n","            ht_new = int(16 * np.ceil(ht_new / 16.0))\n","            input_img = input_img.resize((wd_new, ht_new), PIL.Image.ANTIALIAS)\n","            gt_img = gt_img.resize((wd_new, ht_new), PIL.Image.ANTIALIAS)\n","\n","            return torch.cat([self.transforms(input_img), self.transforms(gt_img)], dim=0), img_id\n","\n","    def __getitem__(self, index):\n","        res = self.get_images(index)\n","        return res\n","\n","    def __len__(self):\n","        return len(self.input_names)"],"metadata":{"id":"5-xx9itPnB7C","executionInfo":{"status":"ok","timestamp":1674819653744,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hùng Phạm Nguyên","userId":"11590122392146338072"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def train_newpaper():\n","  device = \"cuda\"\n","  epochs = 500\n","  lr = 0.0001\n","  model = UNet(c_in = 6).to(device)\n","  optimizer = optim.AdamW(model.parameters(), lr=lr)\n","  mse = nn.MSELoss()\n","  diffusion = Diffusion(img_size=64, device=device)\n","\n","  data = RainDrop()\n","  trainloader = data.get_loaders()\n","  for epoch in range(epochs):\n","    print(\"Starting epoch \", epoch)\n","    pbar = tqdm(trainloader)\n","    for i, (images, _) in enumerate(pbar):\n","      model.train()\n","      input = images.flatten(start_dim=0, end_dim=1)\n","      n = input.size(0)\n","      input = input.to(device)\n","      x = data_transform(input)\n","      e = torch.randn_like(x[:, 3:, :, :])\n","\n","      t = torch.randint(low=0, high=1000, size=(n // 2 + 1,)).to(\"cuda\")\n","      t = torch.cat([t, 1000 - t - 1], dim=0)[:n]\n","\n","      x_noise, noise = diffusion.noise_images(x[:, 3:, :, :], t)\n","      output = model(torch.cat([x[:, :3, :, :], x_noise], dim=1), t.float())\n","      loss = (noise - output).square().sum(dim=(1, 2, 3)).mean(dim=0)\n","\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","      pbar.set_postfix(MSE=loss.item())"],"metadata":{"id":"aTebWsWW1Nnf","executionInfo":{"status":"ok","timestamp":1674819653744,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hùng Phạm Nguyên","userId":"11590122392146338072"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["train_newpaper()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":710},"id":"I_Cl_YSm3q4O","executionInfo":{"status":"error","timestamp":1674819668890,"user_tz":-420,"elapsed":15152,"user":{"displayName":"Hùng Phạm Nguyên","userId":"11590122392146338072"}},"outputId":"e0a12b13-2d14-4916-93f1-82cc9f50670a"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["=> evaluating raindrop test set...\n","/content/drive/MyDrive/Colab Notebooks/DiffusionModel/Dataset/datasettrain/rain\n","96\n","96\n","Starting epoch  0\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/96 [00:08<?, ?it/s]\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-23ad0bf642c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_newpaper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-15-7dce9b5035cd>\u001b[0m in \u001b[0;36mtrain_newpaper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMSE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 14.76 GiB total capacity; 5.65 GiB already allocated; 3.99 GiB free; 9.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]}]}